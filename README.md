# Diqueformer

This project explores diverse sparse and quantum attention mechanisms for graph transformers. Building on the Exphormer architecture, we investigate how varying expander graph connections across layers and epochs can improve robustness and efficiency in graph learning. In parallel, we implement and evaluate quantum-based self-attention using simulation frameworks such as Qiskit and TensorFlow Quantum. The project aims to compare classical, sparse, and quantum transformer variants on molecular property prediction tasks using datasets like QM9 and ZINC.

By Yanjun Zhao, Sami Wolfe, and Shryia Sudhakar
